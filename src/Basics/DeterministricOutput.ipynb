{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac48bc0-f232-4416-a0f2-73f894f251e0",
   "metadata": {},
   "source": [
    "## Proof of Concept to test if we could get deterministic output from Large language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfe634-f91e-4df3-ad8b-a35509096835",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55200733-c69f-41fd-a019-f97ce1e562a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "load_dotenv()\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad83627d-0b2b-4284-b32c-483bd16a4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = r\"../../ca-bundle-full.crt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b308da13-abe6-4f85-8c33-e61429e0def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to tell LLM in how many sentences the output should be generated\n",
    "n_sentences = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21370c-ac02-4e5c-bc1e-baa3929d4923",
   "metadata": {},
   "source": [
    "### Creating LLM model with default configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1313cdd6-570b-47c2-a4f7-f241496c23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c2fe94-b5e7-4b43-94c0-820b2f59e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a32f7b-6bd0-49a5-988b-6a034f77bd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 1 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed before generating the output. By setting a specific seed value, you can ensure that the same sequence of random numbers is used each time you run the LLM. This will result in deterministic output as the generated text will be the same for a given seed. However, note that any changes in the model architecture or input data can still produce different outputs even with the same seed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 1 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b0b9c5-53e1-484d-8a33-8b552e7fe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd7bd15-d21e-4ba8-adeb-912957d34ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 2 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed to a fixed value before generating text. This ensures that the same sequence of random numbers is generated each time. In most programming languages, you can use a random seed function to set the seed value. By using the same seed, you will get the same output from the LLM for a given input. However, keep in mind that the output may still vary if the LLM has any sources of non-determinism, such as random sampling during decoding or the presence of randomness in the underlying model.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 2 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b8892-6c00-4bc1-837d-194ddd5e4926",
   "metadata": {},
   "source": [
    "> #### Conclusion: With default configuration we are getting different output with same meaning or context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ec4d2-3f95-4481-a744-db18b9a0e3cf",
   "metadata": {},
   "source": [
    "### Setting Temperature to 0 to get more reproducable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7dedaa-20c3-470d-9782-d22959fb2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    temperature = 0\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d19da33-6f7e-47d0-bd71-a7a5e5f7df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a44510-501f-4491-8b22-cfe834587008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 1 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed before generating text. This ensures that the same sequence of random numbers is used each time, resulting in consistent output. Additionally, you should disable any sources of randomness within the model, such as dropout or sampling. By controlling the input prompt and model configuration, you can obtain deterministic results from LLMs. However, it's important to note that deterministic output may limit the creativity and diversity of the generated text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 1 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3976eba9-50cc-47b0-8774-3a596c15a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97c3743-3c5b-4504-a6cf-c5656bbdd72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 2 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed before generating text. This ensures that the same sequence of random numbers is used each time, resulting in consistent output. Additionally, you should disable any sources of randomness within the model, such as dropout or sampling. By controlling the input prompt and model configuration, you can obtain deterministic results from LLMs. However, it's important to note that deterministic output may limit the creativity and diversity of the generated text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 2 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526421f-6b22-4afe-90c8-2e359c86f341",
   "metadata": {},
   "source": [
    "> #### Conclusion: By setting temperature to 0, we are getting exactly same output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c369f50-d74d-46cd-ac37-b1e5ccb62406",
   "metadata": {},
   "source": [
    "### Trying with Seed value without modifying temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3624a403-cf9a-4dcd-a8a6-b801c185ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    model_kwargs={\"seed\": 100}\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb31614-87f9-4b8b-a1dd-68792fbd8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "666f4579-1bcf-4faf-9523-6a2094fd9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 1 Output\n",
      "To obtain deterministic output from Language Models (LLMs), you need to set the random seed before running the model. This ensures that all random operations and decisions made during the model's execution are consistent across different runs. Additionally, make sure to disable any sources of randomness within the model architecture, such as dropout or sampling. By controlling the random seed and removing randomness sources, you can achieve deterministic output from LLMs. However, note that deterministic output may limit the model's ability to generate diverse or creative responses.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 1 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e0b4be-eda6-4f33-bf20-95cc1e734413",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e377c0ad-e5ba-4287-9e33-a1429ab56dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 2 Output\n",
      "To obtain deterministic output from Language Models (LLMs), you need to set the random seed before running the model. This ensures that all random operations and decisions made during the model's execution are consistent across different runs. Additionally, make sure to disable any sources of randomness within the model architecture, such as dropout or sampling. By controlling the random seed and removing randomness sources, you can achieve deterministic output from LLMs. However, note that deterministic output may limit the model's ability to generate diverse or creative responses.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 2 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088b8f8-2557-4463-848c-b9236b9af1d9",
   "metadata": {},
   "source": [
    "> #### Conclusion: By setting seed to a constant value, we are getting exactly same output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5af2f1-b5fe-42c4-8f50-18b5abe1ffe6",
   "metadata": {},
   "source": [
    "### Trying with same Seed value as well as with same temperature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e092c30b-f233-49a0-9f90-d26d7a638904",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    temperature = 0,\n",
    "    model_kwargs={\"seed\": 100}\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95cec63c-4391-4a41-a950-575670b90bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7500c2b6-e8c5-4282-b38e-484d2f4d243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 1 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed before generating text. This ensures that the same sequence of random numbers is used each time, resulting in consistent output. Additionally, you should disable any sources of randomness within the model, such as dropout or sampling. By controlling the input prompt and model configuration, you can obtain deterministic results from LLMs. However, it's important to note that deterministic output may limit the creativity and diversity of the generated text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 1 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac91a2d7-6a18-4330-92ac-c9d43f75e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"{instruction}\"\n",
    "message = HumanMessagePromptTemplate.from_template(human_text)\n",
    "prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"instruction\":f\"How to get deterministic output from LLMs. Respond within {n_sentences} sentences\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3f58b41-c545-4edd-ab41-e4e1978fd254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 2 Output\n",
      "To get deterministic output from Language Models (LLMs), you need to set the random seed before generating text. This ensures that the same sequence of random numbers is used each time, resulting in consistent output. Additionally, you should disable any sources of randomness within the model, such as dropout or sampling. By controlling the input prompt and model configuration, you can obtain deterministic results from LLMs. However, it's important to note that deterministic output may limit the creativity and diversity of the generated text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration - 2 Output\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4822c7-9142-4c6c-889f-872df5c168f1",
   "metadata": {},
   "source": [
    "> #### Conclusion: By setting both seed and temperature, we are getting same results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
